{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c88a07-eb9f-4ee3-aabb-e5017a255ce2",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to sort the NCEI storm reports by \"outbreak\" or synoptic case. Here I'll explore methods to try to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d816e3-7417-4cbd-949e-e2ca18ee3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing necessary packages\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns\n",
    "##Add more as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6456ab4-88e7-4c42-8bcb-d4f3345318e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEZONE_MAP = {\n",
    "    # Eastern\n",
    "    \"EST\": \"US/Eastern\", \"EST-5\": \"US/Eastern\", \"EDT\": \"US/Eastern\", \n",
    "    \"E\": \"US/Eastern\", \"ET\": \"US/Eastern\",\n",
    "    \n",
    "    # Central\n",
    "    \"CST\": \"US/Central\", \"CST-6\": \"US/Central\", \"CDT\": \"US/Central\", \n",
    "    \"C\": \"US/Central\", \"CT\": \"US/Central\",\n",
    "    \n",
    "    # Mountain\n",
    "    \"MST\": \"US/Mountain\", \"MST-7\": \"US/Mountain\", \"MDT\": \"US/Mountain\", \n",
    "    \"M\": \"US/Mountain\", \"MT\": \"US/Mountain\",\n",
    "    \n",
    "    # Pacific\n",
    "    \"PST\": \"US/Pacific\", \"PST-8\": \"US/Pacific\", \"PDT\": \"US/Pacific\", \n",
    "    \"P\": \"US/Pacific\", \"PT\": \"US/Pacific\",\n",
    "    \n",
    "    # Alaska / Hawaii / Atlantic\n",
    "    \"AKST\": \"US/Alaska\", \"AST\": \"US/Alaska\", # Note: AST can be Atlantic, but in NCEI context check state\n",
    "    \"HST\": \"US/Hawaii\", \"HAWAII\": \"US/Hawaii\",\n",
    "    \"AST-4\": \"America/Puerto_Rico\", \"ATLANTIC\": \"America/Puerto_Rico\"\n",
    "}\n",
    "\n",
    "def standardize_and_convert_to_utc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the dataframe with 'BEGIN_DT' (Naive) and 'CZ_TIMEZONE',\n",
    "    converts to UTC taking into account DST transitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Clean the CZ_TIMEZONE column\n",
    "    # Remove extra spaces and map to IANA strings\n",
    "    df['clean_tz'] = df['CZ_TIMEZONE'].str.upper().str.strip().map(TIMEZONE_MAP)\n",
    "    \n",
    "    # Fill unknown timezones with UTC to prevent crashing (or drop them)\n",
    "    # Warning: You might want to inspect what falls into 'UTC' later\n",
    "    df['clean_tz'] = df['clean_tz'].fillna('UTC')\n",
    "\n",
    "    # 2. Vectorized Conversion\n",
    "    # We cannot vectorize across mixed timezones, so we group by the timezone.\n",
    "    utc_chunks = []\n",
    "    \n",
    "    # Group by the cleaned timezone string\n",
    "    for tz_name, group in df.groupby('clean_tz'):\n",
    "        if tz_name == 'UTC':\n",
    "            converted = group['BEGIN_DT'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            localized = group['BEGIN_DT'].dt.tz_localize(\n",
    "                tz_name,\n",
    "                ambiguous='NaT',      # fall back (clock goes back) -> mark as missing\n",
    "                nonexistent='shift_forward'  # spring forward -> shift into valid time\n",
    "            )\n",
    "            converted = localized.dt.tz_convert('UTC')\n",
    "        group['BEGIN_DT_UTC'] = converted\n",
    "\n",
    "        # Assign back to the chunk\n",
    "        group['BEGIN_DT_UTC'] = converted\n",
    "        utc_chunks.append(group)\n",
    "\n",
    "    # 3. Reassemble the dataframe\n",
    "    df_utc = pd.concat(utc_chunks).sort_index()\n",
    "    \n",
    "    # Cleanup helper column\n",
    "    return df_utc.drop(columns=['clean_tz'])\n",
    "\n",
    "def build_begin_datetime(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build a reliable datetime from BEGIN_YEARMONTH (YYYYMM), BEGIN_DAY, BEGIN_TIME (HHMM).\n",
    "    Handles missing/odd times by coercing to NaT if needed.\n",
    "    \"\"\"\n",
    "    # Ensure strings with zero-padding\n",
    "    yyyymm = df[\"BEGIN_YEARMONTH\"].astype(\"Int64\").astype(str)          # e.g., 201001\n",
    "    day    = df[\"BEGIN_DAY\"].astype(\"Int64\").astype(str).str.zfill(2)   # e.g., 7 -> \"07\"\n",
    "\n",
    "    # BEGIN_TIME can be 0, 30, 930, 2359, or missing. Pad to 4 digits.\n",
    "    time = (\n",
    "        df[\"BEGIN_TIME\"]\n",
    "        .astype(\"Int64\")\n",
    "        .fillna(0)\n",
    "        .astype(str)\n",
    "        .str.zfill(4)\n",
    "    )\n",
    "\n",
    "    # Concatenate and parse\n",
    "    dt_str = yyyymm + day + time\n",
    "    return pd.to_datetime(dt_str, format=\"%Y%m%d%H%M\", errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffc1807-bad1-4474-b379-75b6d1c869e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 files\n",
      "Reading StormEvents_details-ftp_v1.0_d1950_c20250520.csv.gz (year=1950)\n",
      "Reading StormEvents_details-ftp_v1.0_d1951_c20250520.csv.gz (year=1951)\n",
      "Reading StormEvents_details-ftp_v1.0_d1952_c20250520.csv.gz (year=1952)\n",
      "Reading StormEvents_details-ftp_v1.0_d1953_c20250520.csv.gz (year=1953)\n",
      "Reading StormEvents_details-ftp_v1.0_d1954_c20250520.csv.gz (year=1954)\n",
      "Reading StormEvents_details-ftp_v1.0_d1955_c20250520.csv.gz (year=1955)\n",
      "Reading StormEvents_details-ftp_v1.0_d1956_c20250520.csv.gz (year=1956)\n",
      "Reading StormEvents_details-ftp_v1.0_d1957_c20250520.csv.gz (year=1957)\n",
      "Reading StormEvents_details-ftp_v1.0_d1958_c20250520.csv.gz (year=1958)\n",
      "Reading StormEvents_details-ftp_v1.0_d1959_c20250520.csv.gz (year=1959)\n",
      "Reading StormEvents_details-ftp_v1.0_d1960_c20250520.csv.gz (year=1960)\n",
      "Reading StormEvents_details-ftp_v1.0_d1961_c20250520.csv.gz (year=1961)\n",
      "Reading StormEvents_details-ftp_v1.0_d1962_c20250520.csv.gz (year=1962)\n",
      "Reading StormEvents_details-ftp_v1.0_d1963_c20250520.csv.gz (year=1963)\n",
      "Reading StormEvents_details-ftp_v1.0_d1964_c20250520.csv.gz (year=1964)\n",
      "Reading StormEvents_details-ftp_v1.0_d1965_c20250520.csv.gz (year=1965)\n",
      "Reading StormEvents_details-ftp_v1.0_d1966_c20250520.csv.gz (year=1966)\n",
      "Reading StormEvents_details-ftp_v1.0_d1967_c20250520.csv.gz (year=1967)\n",
      "Reading StormEvents_details-ftp_v1.0_d1968_c20250520.csv.gz (year=1968)\n",
      "Reading StormEvents_details-ftp_v1.0_d1969_c20250520.csv.gz (year=1969)\n",
      "Reading StormEvents_details-ftp_v1.0_d1970_c20250520.csv.gz (year=1970)\n",
      "Reading StormEvents_details-ftp_v1.0_d1971_c20250520.csv.gz (year=1971)\n",
      "Reading StormEvents_details-ftp_v1.0_d1972_c20250520.csv.gz (year=1972)\n",
      "Reading StormEvents_details-ftp_v1.0_d1973_c20250520.csv.gz (year=1973)\n",
      "Reading StormEvents_details-ftp_v1.0_d1974_c20250520.csv.gz (year=1974)\n",
      "Reading StormEvents_details-ftp_v1.0_d1975_c20250520.csv.gz (year=1975)\n",
      "Reading StormEvents_details-ftp_v1.0_d1976_c20250520.csv.gz (year=1976)\n",
      "Reading StormEvents_details-ftp_v1.0_d1977_c20250520.csv.gz (year=1977)\n",
      "Reading StormEvents_details-ftp_v1.0_d1978_c20250520.csv.gz (year=1978)\n",
      "Reading StormEvents_details-ftp_v1.0_d1979_c20250520.csv.gz (year=1979)\n",
      "Reading StormEvents_details-ftp_v1.0_d1980_c20250520.csv.gz (year=1980)\n",
      "Reading StormEvents_details-ftp_v1.0_d1981_c20250520.csv.gz (year=1981)\n",
      "Reading StormEvents_details-ftp_v1.0_d1982_c20250520.csv.gz (year=1982)\n",
      "Reading StormEvents_details-ftp_v1.0_d1983_c20250520.csv.gz (year=1983)\n",
      "Reading StormEvents_details-ftp_v1.0_d1984_c20250520.csv.gz (year=1984)\n",
      "Reading StormEvents_details-ftp_v1.0_d1985_c20250520.csv.gz (year=1985)\n",
      "Reading StormEvents_details-ftp_v1.0_d1986_c20250520.csv.gz (year=1986)\n",
      "Reading StormEvents_details-ftp_v1.0_d1987_c20250520.csv.gz (year=1987)\n",
      "Reading StormEvents_details-ftp_v1.0_d1988_c20250520.csv.gz (year=1988)\n",
      "Reading StormEvents_details-ftp_v1.0_d1989_c20250520.csv.gz (year=1989)\n",
      "Reading StormEvents_details-ftp_v1.0_d1990_c20250520.csv.gz (year=1990)\n",
      "Reading StormEvents_details-ftp_v1.0_d1991_c20250520.csv.gz (year=1991)\n",
      "Reading StormEvents_details-ftp_v1.0_d1992_c20250520.csv.gz (year=1992)\n",
      "Reading StormEvents_details-ftp_v1.0_d1993_c20250520.csv.gz (year=1993)\n",
      "Reading StormEvents_details-ftp_v1.0_d1994_c20250520.csv.gz (year=1994)\n",
      "Reading StormEvents_details-ftp_v1.0_d1995_c20250520.csv.gz (year=1995)\n",
      "Reading StormEvents_details-ftp_v1.0_d1996_c20250520.csv.gz (year=1996)\n",
      "Reading StormEvents_details-ftp_v1.0_d1997_c20250520.csv.gz (year=1997)\n",
      "Reading StormEvents_details-ftp_v1.0_d1998_c20250520.csv.gz (year=1998)\n",
      "Reading StormEvents_details-ftp_v1.0_d1999_c20250520.csv.gz (year=1999)\n",
      "Reading StormEvents_details-ftp_v1.0_d2000_c20250520.csv.gz (year=2000)\n",
      "Reading StormEvents_details-ftp_v1.0_d2001_c20250520.csv.gz (year=2001)\n",
      "Reading StormEvents_details-ftp_v1.0_d2002_c20250520.csv.gz (year=2002)\n",
      "Reading StormEvents_details-ftp_v1.0_d2003_c20250520.csv.gz (year=2003)\n",
      "Reading StormEvents_details-ftp_v1.0_d2004_c20250520.csv.gz (year=2004)\n",
      "Reading StormEvents_details-ftp_v1.0_d2005_c20250520.csv.gz (year=2005)\n",
      "Reading StormEvents_details-ftp_v1.0_d2006_c20250520.csv.gz (year=2006)\n",
      "Reading StormEvents_details-ftp_v1.0_d2007_c20250520.csv.gz (year=2007)\n",
      "Reading StormEvents_details-ftp_v1.0_d2008_c20251204.csv.gz (year=2008)\n",
      "Reading StormEvents_details-ftp_v1.0_d2009_c20250520.csv.gz (year=2009)\n",
      "Reading StormEvents_details-ftp_v1.0_d2010_c20250520.csv.gz (year=2010)\n",
      "Reading StormEvents_details-ftp_v1.0_d2011_c20250520.csv.gz (year=2011)\n",
      "Reading StormEvents_details-ftp_v1.0_d2012_c20250520.csv.gz (year=2012)\n",
      "Reading StormEvents_details-ftp_v1.0_d2013_c20250520.csv.gz (year=2013)\n",
      "Reading StormEvents_details-ftp_v1.0_d2014_c20250520.csv.gz (year=2014)\n",
      "Reading StormEvents_details-ftp_v1.0_d2015_c20251118.csv.gz (year=2015)\n",
      "Reading StormEvents_details-ftp_v1.0_d2016_c20250818.csv.gz (year=2016)\n",
      "Reading StormEvents_details-ftp_v1.0_d2017_c20260116.csv.gz (year=2017)\n",
      "Reading StormEvents_details-ftp_v1.0_d2018_c20260116.csv.gz (year=2018)\n",
      "Reading StormEvents_details-ftp_v1.0_d2019_c20260116.csv.gz (year=2019)\n",
      "Reading StormEvents_details-ftp_v1.0_d2020_c20260116.csv.gz (year=2020)\n",
      "Reading StormEvents_details-ftp_v1.0_d2021_c20250520.csv.gz (year=2021)\n",
      "Reading StormEvents_details-ftp_v1.0_d2022_c20250721.csv.gz (year=2022)\n",
      "Reading StormEvents_details-ftp_v1.0_d2023_c20260116.csv.gz (year=2023)\n",
      "Reading StormEvents_details-ftp_v1.0_d2024_c20260116.csv.gz (year=2024)\n",
      "Reading StormEvents_details-ftp_v1.0_d2025_c20260116.csv.gz (year=2025)\n",
      "Done.\n",
      "(109269, 55)\n",
      "EVENT_TYPE\n",
      "Thunderstorm Wind    52365\n",
      "Hail                 43676\n",
      "Tornado              13228\n",
      "Name: count, dtype: int64\n",
      "1950 2024\n"
     ]
    }
   ],
   "source": [
    "# Loading in the data\n",
    "data_dir = Path(\"/data1/lepique/stormevents_details/\")  \n",
    "pattern = \"StormEvents_details-ftp_v1.0_d*_c*.csv.gz\"\n",
    "\n",
    "storm_types = {\"Tornado\", \"Hail\", \"Thunderstorm Wind\"}\n",
    "winter_months = {12, 1, 2, 3}\n",
    "\n",
    "\n",
    "### Keeping only the variables of immediate interest (can modify if needed)\n",
    "keep_cols = [\n",
    "    \"EVENT_ID\", \"EPISODE_ID\", \"EVENT_TYPE\",\n",
    "    \"BEGIN_DT\",\n",
    "    \"STATE\", \"WFO\",\n",
    "    \"BEGIN_LAT\", \"BEGIN_LON\",\n",
    "    \"MAGNITUDE\", \"MAGNITUDE_TYPE\",\n",
    "]\n",
    "\n",
    "\n",
    "# --- Find all yearly files ---\n",
    "files = sorted(data_dir.glob(pattern))\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for fp in files:\n",
    "    # Extract the year from filename (the 'dYYYY' part)\n",
    "    m = re.search(r\"_d(\\d{4})_\", fp.name)\n",
    "    year = int(m.group(1)) if m else None\n",
    "\n",
    "    print(f\"Reading {fp.name} (year={year})\")\n",
    "\n",
    "    df = pd.read_csv(fp, compression=\"gzip\", low_memory=False)\n",
    "\n",
    "    # Ensure datetime is datetime (works even if already parsed)\n",
    "    df[\"BEGIN_DT\"] = build_begin_datetime(df)\n",
    "    df = df.dropna(subset=[\"BEGIN_DT\"])\n",
    "\n",
    "\n",
    "    # Drop rows with missing begin time (rare but safer for dt.month)\n",
    "    df = df.dropna(subset=[\"BEGIN_DATE_TIME\"])\n",
    "\n",
    "    # DJFM filter\n",
    "    df = df[df[\"BEGIN_DT\"].dt.month.isin(winter_months)]\n",
    "    \n",
    "    # December-year winter label\n",
    "    dt = df[\"BEGIN_DT\"]\n",
    "    df[\"WINTER_SEASON\"] = dt.dt.year - dt.dt.month.isin(winter_months).astype(int)\n",
    "\n",
    "    \n",
    "\n",
    "    # Storm-type filter\n",
    "    df = df[df[\"EVENT_TYPE\"].isin(storm_types)]\n",
    "\n",
    "    # Keep only the columns we care about (plus WINTER_SEASON)\n",
    "#    cols = [\"WINTER_SEASON\"] + keep_cols\n",
    "#    df = df[cols].copy()\n",
    "\n",
    "    # Optional: also store source year (from filename) to help debugging\n",
    "    df[\"SOURCE_FILE_YEAR\"] = year\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# --- Concatenate all years ---\n",
    "df_winter_scs = pd.concat(dfs, ignore_index=True)\n",
    "df_winter_scs = df_winter_scs[df_winter_scs[\"WINTER_SEASON\"] >= 1950]  ## Eliminating JFM 1950 because we don't have full winter season\n",
    "\n",
    "\n",
    "# Adding a UTC Time column\n",
    "df_winter_scs = standardize_and_convert_to_utc(df_winter_scs)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(df_winter_scs.shape)\n",
    "print(df_winter_scs[\"EVENT_TYPE\"].value_counts())\n",
    "print(df_winter_scs[\"WINTER_SEASON\"].min(), df_winter_scs[\"WINTER_SEASON\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c78eec-7615-4651-a843-e903fbdc6040",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df_winter_scs[df_winter_scs[\"WINTER_SEASON\"] >= 1995]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25663f26-2f11-4bc8-9b17-e7ca0d406c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synoptic_cases(df, gap_threshold_hours=12):\n",
    "    \"\"\"\n",
    "    Groups NCEI Episodes into Synoptic Cases based on a time gap threshold.\n",
    "    \n",
    "    Args:\n",
    "        df: The dataframe of storm reports.\n",
    "        gap_threshold_hours: Hours of 'silence' required to start a new case.\n",
    "                             12 hours is standard for synoptic separation.\n",
    "                             6 hours is strict. \n",
    "                             72 hours is common for Reinsurance 'Event' clauses.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Isolate unique Episodes first\n",
    "    # We group by EPISODE_ID to find the earliest start and latest start of that specific episode\n",
    "    # (Using BEGIN_DT for both to keep it simple, or create END_DT if available)\n",
    "    episodes = df.groupby('EPISODE_ID', as_index=False).agg({\n",
    "        'BEGIN_DT_UTC': 'min', \n",
    "        'BEGIN_LAT': 'mean',\n",
    "        'BEGIN_LON': 'mean'\n",
    "    }).sort_values('BEGIN_DT_UTC')\n",
    "\n",
    "    threshold = pd.Timedelta(hours=gap_threshold_hours)\n",
    "\n",
    "    \n",
    "     # 2. Walk forward in time, comparing to the *start* of the current case\n",
    "    case_ids = []\n",
    "    current_case_id = 0\n",
    "    current_case_start = None\n",
    "\n",
    "    for t in episodes['BEGIN_DT_UTC']:\n",
    "        if current_case_start is None or (t - current_case_start) > threshold:\n",
    "            # Start a new synoptic case\n",
    "            current_case_id += 1\n",
    "            current_case_start = t\n",
    "        case_ids.append(current_case_id)\n",
    "\n",
    "    episodes['SYNOPTIC_CASE_ID'] = case_ids\n",
    "\n",
    "    # 3. Merge back to the full dataframe\n",
    "    return df.merge(\n",
    "        episodes[['EPISODE_ID', 'SYNOPTIC_CASE_ID']],\n",
    "        on='EPISODE_ID',\n",
    "        how='left',\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315ebc5d-1f5d-4773-b1c0-df59e126b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Episodes: 22051\n",
      "Total Synoptic Cases: 2128\n"
     ]
    }
   ],
   "source": [
    "# --- Run the function ---\n",
    "# A 12-hour gap is a solid meteorological definition for separating synoptic waves.\n",
    "# If reports stop for 12 hours, the next report is likely a new system.\n",
    "subset = create_synoptic_cases(subset, gap_threshold_hours=12)\n",
    "\n",
    "print(f\"Total Unique Episodes: {subset['EPISODE_ID'].nunique()}\")\n",
    "print(f\"Total Synoptic Cases: {subset['SYNOPTIC_CASE_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5641e4f6-e5a7-4cf6-8f81-85ccdf773f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85214\n",
      "85214\n"
     ]
    }
   ],
   "source": [
    "print(len(subset))\n",
    "print(subset['EVENT_ID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793cd8d-6c7b-449e-91a5-fefad521d68c",
   "metadata": {},
   "source": [
    "So each event ID is unique across time, so I can sort by event ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c442429-f2b9-4d5e-8a29-69e6cdacf555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_case_stats(df):\n",
    "    \"\"\"\n",
    "    Create a new dataframe of synoptic case statistics to analyze\n",
    "    \"\"\"\n",
    "    \n",
    "    case_stats = df.groupby('SYNOPTIC_CASE_ID').agg({\n",
    "    'WINTER_SEASON': 'first',             # The season this case belongs to\n",
    "    'BEGIN_DT': ['min', 'max'],           # Start/End times\n",
    "    'EVENT_TYPE': 'count',                # Total reports\n",
    "    'BEGIN_LAT': 'mean',                  # Centroid Lat\n",
    "    'BEGIN_LON': 'mean',                  # Centroid Lon\n",
    "    'EVENT_ID': 'nunique'                 # Double check unique events\n",
    "})\n",
    "    \n",
    "    case_stats.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in case_stats.columns.values]\n",
    "    case_stats = case_stats.rename(columns={'WINTER_SEASON_first': 'Season', 'EVENT_TYPE_count': 'Report_Count'})\n",
    "    \n",
    "    # Calculate Duration (in Hours)\n",
    "    case_stats['Duration_Hrs'] = (case_stats['BEGIN_DT_max'] - case_stats['BEGIN_DT_min']).dt.total_seconds() / 3600\n",
    "\n",
    "    print(f\"Total Synoptic Cases: {len(case_stats)}\")\n",
    "    \n",
    "    return case_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d7c1604-70cf-4b8a-b896-4815b13cdf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Synoptic Cases: 2128\n"
     ]
    }
   ],
   "source": [
    "case_stats = create_case_stats(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4223bc4d-5d9a-45c5-ab36-d5dd41dc57ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(680.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_stats['Duration_Hrs'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24359fc0-d9d8-46f5-8a70-84b762c4d349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59eccd4-5aa4-4a53-802d-26e01d520fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
