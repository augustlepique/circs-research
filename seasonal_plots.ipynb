{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c98f94a",
   "metadata": {},
   "source": [
    "# This notebook is to plot geographical density of SCS counts over a given season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e39219",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing necessary packages\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80f5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEZONE_MAP = {\n",
    "    # Eastern\n",
    "    \"EST\": \"US/Eastern\", \"EST-5\": \"US/Eastern\", \"EDT\": \"US/Eastern\",\n",
    "    \"E\": \"US/Eastern\", \"ET\": \"US/Eastern\",\n",
    "\n",
    "    # Central\n",
    "    \"CST\": \"US/Central\", \"CST-6\": \"US/Central\", \"CDT\": \"US/Central\",\n",
    "    \"C\": \"US/Central\", \"CT\": \"US/Central\",\n",
    "\n",
    "    # Mountain\n",
    "    \"MST\": \"US/Mountain\", \"MST-7\": \"US/Mountain\", \"MDT\": \"US/Mountain\",\n",
    "    \"M\": \"US/Mountain\", \"MT\": \"US/Mountain\",\n",
    "\n",
    "    # Pacific\n",
    "    \"PST\": \"US/Pacific\", \"PST-8\": \"US/Pacific\", \"PDT\": \"US/Pacific\",\n",
    "    \"P\": \"US/Pacific\", \"PT\": \"US/Pacific\",\n",
    "\n",
    "    # Alaska / Hawaii / Atlantic\n",
    "    \"AKST\": \"US/Alaska\", \"AST\": \"US/Alaska\",  # AST can be Atlantic; check state if needed\n",
    "    \"HST\": \"US/Hawaii\", \"HAWAII\": \"US/Hawaii\",\n",
    "    \"AST-4\": \"America/Puerto_Rico\", \"ATLANTIC\": \"America/Puerto_Rico\"\n",
    "}\n",
    "\n",
    "\n",
    "def build_begin_datetime(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build a reliable datetime from BEGIN_YEARMONTH (YYYYMM),\n",
    "    BEGIN_DAY, BEGIN_TIME (HHMM). Coerces bad values to NaT.\n",
    "    \"\"\"\n",
    "    yyyymm = df[\"BEGIN_YEARMONTH\"].astype(\"Int64\").astype(str)\n",
    "    day = df[\"BEGIN_DAY\"].astype(\"Int64\").astype(str).str.zfill(2)\n",
    "\n",
    "    time = (\n",
    "        df[\"BEGIN_TIME\"]\n",
    "        .astype(\"Int64\")\n",
    "        .fillna(0)\n",
    "        .astype(str)\n",
    "        .str.zfill(4)\n",
    "    )\n",
    "\n",
    "    dt_str = yyyymm + day + time\n",
    "    return pd.to_datetime(dt_str, format=\"%Y%m%d%H%M\", errors=\"coerce\")\n",
    "\n",
    "\n",
    "def standardize_and_convert_to_utc(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the dataframe with 'BEGIN_DT' (naive) and 'CZ_TIMEZONE',\n",
    "    converts to UTC taking into account DST transitions.\n",
    "    \"\"\"\n",
    "    df['clean_tz'] = df['CZ_TIMEZONE'].str.upper().str.strip().map(TIMEZONE_MAP)\n",
    "    df['clean_tz'] = df['clean_tz'].fillna('UTC')\n",
    "\n",
    "    utc_chunks = []\n",
    "\n",
    "    for tz_name, group in df.groupby('clean_tz'):\n",
    "        if tz_name == 'UTC':\n",
    "            converted = group['BEGIN_DT'].dt.tz_localize('UTC')\n",
    "        else:\n",
    "            localized = group['BEGIN_DT'].dt.tz_localize(\n",
    "                tz_name,\n",
    "                ambiguous='NaT',\n",
    "                nonexistent='shift_forward'\n",
    "            )\n",
    "            converted = localized.dt.tz_convert('UTC')\n",
    "\n",
    "        group['BEGIN_DT_UTC'] = converted\n",
    "        utc_chunks.append(group)\n",
    "\n",
    "    df_utc = pd.concat(utc_chunks).sort_index()\n",
    "    return df_utc.drop(columns=['clean_tz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1165722c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 files\n",
      "Reading StormEvents_details-ftp_v1.0_d1950_c20250520.csv.gz (year=1950)\n",
      "Reading StormEvents_details-ftp_v1.0_d1951_c20250520.csv.gz (year=1951)\n",
      "Reading StormEvents_details-ftp_v1.0_d1952_c20250520.csv.gz (year=1952)\n",
      "Reading StormEvents_details-ftp_v1.0_d1953_c20250520.csv.gz (year=1953)\n",
      "Reading StormEvents_details-ftp_v1.0_d1954_c20250520.csv.gz (year=1954)\n",
      "Reading StormEvents_details-ftp_v1.0_d1955_c20250520.csv.gz (year=1955)\n",
      "Reading StormEvents_details-ftp_v1.0_d1956_c20250520.csv.gz (year=1956)\n",
      "Reading StormEvents_details-ftp_v1.0_d1957_c20250520.csv.gz (year=1957)\n",
      "Reading StormEvents_details-ftp_v1.0_d1958_c20250520.csv.gz (year=1958)\n",
      "Reading StormEvents_details-ftp_v1.0_d1959_c20250520.csv.gz (year=1959)\n",
      "Reading StormEvents_details-ftp_v1.0_d1960_c20250520.csv.gz (year=1960)\n",
      "Reading StormEvents_details-ftp_v1.0_d1961_c20250520.csv.gz (year=1961)\n",
      "Reading StormEvents_details-ftp_v1.0_d1962_c20250520.csv.gz (year=1962)\n",
      "Reading StormEvents_details-ftp_v1.0_d1963_c20250520.csv.gz (year=1963)\n",
      "Reading StormEvents_details-ftp_v1.0_d1964_c20250520.csv.gz (year=1964)\n",
      "Reading StormEvents_details-ftp_v1.0_d1965_c20250520.csv.gz (year=1965)\n",
      "Reading StormEvents_details-ftp_v1.0_d1966_c20250520.csv.gz (year=1966)\n",
      "Reading StormEvents_details-ftp_v1.0_d1967_c20250520.csv.gz (year=1967)\n",
      "Reading StormEvents_details-ftp_v1.0_d1968_c20250520.csv.gz (year=1968)\n",
      "Reading StormEvents_details-ftp_v1.0_d1969_c20250520.csv.gz (year=1969)\n",
      "Reading StormEvents_details-ftp_v1.0_d1970_c20250520.csv.gz (year=1970)\n",
      "Reading StormEvents_details-ftp_v1.0_d1971_c20250520.csv.gz (year=1971)\n",
      "Reading StormEvents_details-ftp_v1.0_d1972_c20250520.csv.gz (year=1972)\n",
      "Reading StormEvents_details-ftp_v1.0_d1973_c20250520.csv.gz (year=1973)\n",
      "Reading StormEvents_details-ftp_v1.0_d1974_c20250520.csv.gz (year=1974)\n",
      "Reading StormEvents_details-ftp_v1.0_d1975_c20250520.csv.gz (year=1975)\n",
      "Reading StormEvents_details-ftp_v1.0_d1976_c20250520.csv.gz (year=1976)\n",
      "Reading StormEvents_details-ftp_v1.0_d1977_c20250520.csv.gz (year=1977)\n",
      "Reading StormEvents_details-ftp_v1.0_d1978_c20250520.csv.gz (year=1978)\n",
      "Reading StormEvents_details-ftp_v1.0_d1979_c20250520.csv.gz (year=1979)\n",
      "Reading StormEvents_details-ftp_v1.0_d1980_c20250520.csv.gz (year=1980)\n",
      "Reading StormEvents_details-ftp_v1.0_d1981_c20250520.csv.gz (year=1981)\n",
      "Reading StormEvents_details-ftp_v1.0_d1982_c20250520.csv.gz (year=1982)\n",
      "Reading StormEvents_details-ftp_v1.0_d1983_c20250520.csv.gz (year=1983)\n",
      "Reading StormEvents_details-ftp_v1.0_d1984_c20250520.csv.gz (year=1984)\n",
      "Reading StormEvents_details-ftp_v1.0_d1985_c20250520.csv.gz (year=1985)\n",
      "Reading StormEvents_details-ftp_v1.0_d1986_c20250520.csv.gz (year=1986)\n",
      "Reading StormEvents_details-ftp_v1.0_d1987_c20250520.csv.gz (year=1987)\n",
      "Reading StormEvents_details-ftp_v1.0_d1988_c20250520.csv.gz (year=1988)\n",
      "Reading StormEvents_details-ftp_v1.0_d1989_c20250520.csv.gz (year=1989)\n",
      "Reading StormEvents_details-ftp_v1.0_d1990_c20250520.csv.gz (year=1990)\n",
      "Reading StormEvents_details-ftp_v1.0_d1991_c20250520.csv.gz (year=1991)\n",
      "Reading StormEvents_details-ftp_v1.0_d1992_c20250520.csv.gz (year=1992)\n",
      "Reading StormEvents_details-ftp_v1.0_d1993_c20250520.csv.gz (year=1993)\n",
      "Reading StormEvents_details-ftp_v1.0_d1994_c20250520.csv.gz (year=1994)\n",
      "Reading StormEvents_details-ftp_v1.0_d1995_c20250520.csv.gz (year=1995)\n",
      "Reading StormEvents_details-ftp_v1.0_d1996_c20250520.csv.gz (year=1996)\n",
      "Reading StormEvents_details-ftp_v1.0_d1997_c20250520.csv.gz (year=1997)\n",
      "Reading StormEvents_details-ftp_v1.0_d1998_c20250520.csv.gz (year=1998)\n",
      "Reading StormEvents_details-ftp_v1.0_d1999_c20250520.csv.gz (year=1999)\n",
      "Reading StormEvents_details-ftp_v1.0_d2000_c20250520.csv.gz (year=2000)\n",
      "Reading StormEvents_details-ftp_v1.0_d2001_c20250520.csv.gz (year=2001)\n",
      "Reading StormEvents_details-ftp_v1.0_d2002_c20250520.csv.gz (year=2002)\n",
      "Reading StormEvents_details-ftp_v1.0_d2003_c20250520.csv.gz (year=2003)\n",
      "Reading StormEvents_details-ftp_v1.0_d2004_c20250520.csv.gz (year=2004)\n",
      "Reading StormEvents_details-ftp_v1.0_d2005_c20250520.csv.gz (year=2005)\n",
      "Reading StormEvents_details-ftp_v1.0_d2006_c20250520.csv.gz (year=2006)\n",
      "Reading StormEvents_details-ftp_v1.0_d2007_c20250520.csv.gz (year=2007)\n",
      "Reading StormEvents_details-ftp_v1.0_d2008_c20251204.csv.gz (year=2008)\n",
      "Reading StormEvents_details-ftp_v1.0_d2009_c20250520.csv.gz (year=2009)\n",
      "Reading StormEvents_details-ftp_v1.0_d2010_c20250520.csv.gz (year=2010)\n",
      "Reading StormEvents_details-ftp_v1.0_d2011_c20250520.csv.gz (year=2011)\n",
      "Reading StormEvents_details-ftp_v1.0_d2012_c20250520.csv.gz (year=2012)\n",
      "Reading StormEvents_details-ftp_v1.0_d2013_c20250520.csv.gz (year=2013)\n",
      "Reading StormEvents_details-ftp_v1.0_d2014_c20250520.csv.gz (year=2014)\n",
      "Reading StormEvents_details-ftp_v1.0_d2015_c20251118.csv.gz (year=2015)\n",
      "Reading StormEvents_details-ftp_v1.0_d2016_c20250818.csv.gz (year=2016)\n",
      "Reading StormEvents_details-ftp_v1.0_d2017_c20260116.csv.gz (year=2017)\n",
      "Reading StormEvents_details-ftp_v1.0_d2018_c20260116.csv.gz (year=2018)\n",
      "Reading StormEvents_details-ftp_v1.0_d2019_c20260116.csv.gz (year=2019)\n",
      "Reading StormEvents_details-ftp_v1.0_d2020_c20260116.csv.gz (year=2020)\n",
      "Reading StormEvents_details-ftp_v1.0_d2021_c20250520.csv.gz (year=2021)\n",
      "Reading StormEvents_details-ftp_v1.0_d2022_c20250721.csv.gz (year=2022)\n",
      "Reading StormEvents_details-ftp_v1.0_d2023_c20260116.csv.gz (year=2023)\n",
      "Reading StormEvents_details-ftp_v1.0_d2024_c20260116.csv.gz (year=2024)\n",
      "Reading StormEvents_details-ftp_v1.0_d2025_c20260116.csv.gz (year=2025)\n",
      "Done.\n",
      "(109269, 55)\n",
      "EVENT_TYPE\n",
      "Thunderstorm Wind    52365\n",
      "Hail                 43676\n",
      "Tornado              13228\n",
      "Name: count, dtype: int64\n",
      "1950 2024\n"
     ]
    }
   ],
   "source": [
    "# Loading in the data\n",
    "data_dir = Path(\"/data1/lepique/stormevents_details/\")  # adjust if needed\n",
    "pattern = \"StormEvents_details-ftp_v1.0_d*_c*.csv.gz\"\n",
    "\n",
    "storm_types = {\"Tornado\", \"Hail\", \"Thunderstorm Wind\"}\n",
    "winter_months = {12, 1, 2, 3}\n",
    "\n",
    "# Keeping only variables of immediate interest (can modify if needed)\n",
    "keep_cols = [\n",
    "    \"EVENT_ID\", \"EPISODE_ID\", \"EVENT_TYPE\",\n",
    "    \"BEGIN_DT\",\n",
    "    \"STATE\", \"WFO\",\n",
    "    \"BEGIN_LAT\", \"BEGIN_LON\",\n",
    "    \"MAGNITUDE\", \"MAGNITUDE_TYPE\",\n",
    "]\n",
    "\n",
    "files = sorted(data_dir.glob(pattern))\n",
    "print(f\"Found {len(files)} files\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for fp in files:\n",
    "    m = re.search(r\"_d(\\d{4})_\", fp.name)\n",
    "    year = int(m.group(1)) if m else None\n",
    "\n",
    "    print(f\"Reading {fp.name} (year={year})\")\n",
    "\n",
    "    df = pd.read_csv(fp, compression=\"gzip\", low_memory=False)\n",
    "\n",
    "    # Ensure datetime is datetime\n",
    "    df[\"BEGIN_DT\"] = build_begin_datetime(df)\n",
    "    df = df.dropna(subset=[\"BEGIN_DT\"])\n",
    "\n",
    "    # Drop rows with missing begin time (safer for dt.month)\n",
    "    df = df.dropna(subset=[\"BEGIN_DATE_TIME\"])\n",
    "\n",
    "    # DJFM filter\n",
    "    df = df[df[\"BEGIN_DT\"].dt.month.isin(winter_months)]\n",
    "\n",
    "    # December-year winter label\n",
    "    dt = df[\"BEGIN_DT\"]\n",
    "    df[\"WINTER_SEASON\"] = dt.dt.year - dt.dt.month.isin(winter_months).astype(int)\n",
    "\n",
    "    # Storm-type filter\n",
    "    df = df[df[\"EVENT_TYPE\"].isin(storm_types)]\n",
    "\n",
    "    # Optional: keep only selected columns\n",
    "    # cols = [\"WINTER_SEASON\"] + keep_cols\n",
    "    # df = df[cols].copy()\n",
    "\n",
    "    # Optional: source file year\n",
    "    df[\"SOURCE_FILE_YEAR\"] = year\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all years\n",
    "df_winter_scs = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Eliminate JFM 1950 because full winter season is not available\n",
    "df_winter_scs = df_winter_scs[df_winter_scs[\"WINTER_SEASON\"] >= 1950]\n",
    "\n",
    "# Add UTC time column\n",
    "df_winter_scs = standardize_and_convert_to_utc(df_winter_scs)\n",
    "\n",
    "print(\"Done.\")\n",
    "print(df_winter_scs.shape)\n",
    "print(df_winter_scs[\"EVENT_TYPE\"].value_counts())\n",
    "print(df_winter_scs[\"WINTER_SEASON\"].min(), df_winter_scs[\"WINTER_SEASON\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41d27c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total DJFM reports in winter season 2010: 3068\n",
      "EVENT_TYPE\n",
      "Thunderstorm Wind    1442\n",
      "Hail                 1435\n",
      "Tornado               191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Choose the DJFM winter season (December-year label)\n",
    "# Example: 2010 means Dec 2010 – Mar 2011\n",
    "season = 2010\n",
    "\n",
    "season_subset = df_winter_scs[df_winter_scs[\"WINTER_SEASON\"] == season]\n",
    "print(f\"Total DJFM reports in winter season {season}: {len(season_subset)}\")\n",
    "print(season_subset[\"EVENT_TYPE\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff2072a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_season_density(df, season, event_type, bins=60, save_path=None):\n",
    "    \"\"\"\n",
    "    Full-US cartopy map for one winter season and hazard type.\n",
    "    Uses same projection/extent as plot_case, but shades density.\n",
    "    \"\"\"\n",
    "    # Filter to this season + hazard\n",
    "    et = df[\"EVENT_TYPE\"].str.upper().fillna(\"OTHER\")\n",
    "    if event_type.lower().startswith(\"tornado\"):\n",
    "        mask = et.str.contains(\"TORNADO\")\n",
    "        cmap = \"Reds\"\n",
    "    elif event_type.lower().startswith(\"hail\"):\n",
    "        mask = et.str.contains(\"HAIL\")\n",
    "        cmap = \"Greens\"\n",
    "    else:  # Thunderstorm Wind\n",
    "        mask = et.str.contains(\"THUNDERSTORM WIND\") | et.str.contains(\"TSTM WIND\")\n",
    "        cmap = \"Blues\"\n",
    "\n",
    "    season_df = df[(df[\"WINTER_SEASON\"] == season) & mask].copy()\n",
    "    season_df = season_df.dropna(subset=[\"BEGIN_LAT\", \"BEGIN_LON\"])\n",
    "\n",
    "    if season_df.empty:\n",
    "        print(f\"No {event_type} reports for WINTER_SEASON={season}\")\n",
    "        return\n",
    "\n",
    "    # Same projection and CONUS extent as plot_case\n",
    "    proj = ccrs.LambertConformal(\n",
    "        central_longitude=-96, central_latitude=39,\n",
    "        standard_parallels=(33, 45),\n",
    "    )\n",
    "    extent = [-130, -65, 23, 50]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = plt.axes(projection=proj)\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Basemap features\n",
    "    ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth=0.6)\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale('50m'), linewidth=0.6)\n",
    "    ax.add_feature(cfeature.STATES.with_scale('50m'), linewidth=0.4)\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    # Fixed grid over CONUS to avoid weird artifacts\n",
    "    ### This currently bins by degrees, but I would like to try binning by county, or area instead. \n",
    "    lon_min, lon_max, lat_min, lat_max = extent\n",
    "    lon_bins = np.linspace(lon_min, lon_max, bins + 1)\n",
    "    lat_bins = np.linspace(lat_min, lat_max, bins + 1)\n",
    "\n",
    "    H, lon_edges, lat_edges = np.histogram2d(\n",
    "        season_df[\"BEGIN_LON\"].values,\n",
    "        season_df[\"BEGIN_LAT\"].values,\n",
    "        bins=[lon_bins, lat_bins],\n",
    "    )\n",
    "\n",
    "    # Mask zero-count cells so background stays clean\n",
    "    H_masked = np.ma.masked_where(H.T == 0, H.T)\n",
    "\n",
    "    pcm = ax.pcolormesh(\n",
    "        lon_edges,\n",
    "        lat_edges,\n",
    "        H_masked,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap=cmap,\n",
    "        shading=\"auto\",\n",
    "    )\n",
    "    pcm.cmap.set_bad(\"none\")  # masked cells transparent\n",
    "\n",
    "    cb = plt.colorbar(pcm, ax=ax, orientation=\"vertical\", pad=0.02, shrink=0.7)\n",
    "    cb.set_label(\"Report count per grid cell\")\n",
    "\n",
    "    ax.set_title(f\"{event_type} density\\nDJFM winter season {season}\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path = os.path.expanduser(save_path)\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved to: {os.path.abspath(save_path)}\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home1/lepique/circs_research/1990_Thunderstorm Wind.png\n",
      "Saved to: /home1/lepique/circs_research/1990_Hail.png\n",
      "Saved to: /home1/lepique/circs_research/1990_Tornado.png\n"
     ]
    }
   ],
   "source": [
    "for hazard in [\"Thunderstorm_Wind\", \"Hail\", \"Tornado\"]:\n",
    "    plot_season_density(df_winter_scs, season=1990, event_type=hazard, bins=60, save_path='~/circs_research/1990_'+hazard+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a06fccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home1/lepique/circs_research/hail_density_2009.png\n"
     ]
    }
   ],
   "source": [
    "plot_season_density(\n",
    "df_winter_scs, \n",
    "season=2009, \n",
    "event_type='Hail', \n",
    "save_path='~/circs_research/hail_density_2009.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f80ff641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniqueCountsResult(values=array(['C', 'Z'], dtype=object), counts=array([109227,     42]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique_counts((df_winter_scs['CZ_TYPE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f46f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## subsetting to only counties and not marine zones\n",
    "df_winter_scs = df_winter_scs[df_winter_scs[\"CZ_TYPE\"] == \"C\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb381d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting state and county FIPS codes to strings so that I can combine them together to create unique COUNTY_FIPS codes\n",
    "df_winter_scs =df_winter_scs[df_winter_scs[\"CZ_TYPE\"] == \"C\"].copy()\n",
    "\n",
    "df_winter_scs[\"STATE_FIPS\"] = (\n",
    "   df_winter_scs[\"STATE_FIPS\"]\n",
    "    .astype(float)\n",
    "    .astype(int)\n",
    "    .astype(str)\n",
    "    .str.zfill(2)\n",
    ")\n",
    "\n",
    "df_winter_scs[\"CZ_FIPS\"] = (\n",
    "   df_winter_scs[\"CZ_FIPS\"]\n",
    "    .astype(float)\n",
    "    .astype(int)\n",
    "    .astype(str)\n",
    "    .str.zfill(3)\n",
    ")\n",
    "\n",
    "df_winter_scs[\"COUNTY_FIPS\"] = (\n",
    "   df_winter_scs[\"STATE_FIPS\"] +df_winter_scs[\"CZ_FIPS\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec7d8cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48225\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "#Verification:\n",
    "print(df_winter_scs['COUNTY_FIPS'].iloc[0])\n",
    "print(df_winter_scs[\"COUNTY_FIPS\"].str.len().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e808ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.groupby.generic.DataFrameGroupBy'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_winter_scs))\n",
    "print(type(df_winter_scs['COUNTY_FIPS']))\n",
    "print(type(df_winter_scs.groupby([\"COUNTY_FIPS\", \"EVENT_TYPE\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c90a6668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>BEGIN_DT</th>\n",
       "      <th>WINTER_SEASON</th>\n",
       "      <th>SOURCE_FILE_YEAR</th>\n",
       "      <th>BEGIN_DT_UTC</th>\n",
       "      <th>COUNTY_FIPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>195103</td>\n",
       "      <td>28</td>\n",
       "      <td>510</td>\n",
       "      <td>195103</td>\n",
       "      <td>28</td>\n",
       "      <td>510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10120421</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PUB</td>\n",
       "      <td>1951-03-28 05:10:00</td>\n",
       "      <td>1950</td>\n",
       "      <td>1951</td>\n",
       "      <td>1951-03-28 11:10:00+00:00</td>\n",
       "      <td>48225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>195103</td>\n",
       "      <td>30</td>\n",
       "      <td>1500</td>\n",
       "      <td>195103</td>\n",
       "      <td>30</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10104933</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PUB</td>\n",
       "      <td>1951-03-30 15:00:00</td>\n",
       "      <td>1950</td>\n",
       "      <td>1951</td>\n",
       "      <td>1951-03-30 21:00:00+00:00</td>\n",
       "      <td>42001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>195102</td>\n",
       "      <td>19</td>\n",
       "      <td>1830</td>\n",
       "      <td>195102</td>\n",
       "      <td>19</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10099493</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>35.37</td>\n",
       "      <td>-98.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PUB</td>\n",
       "      <td>1951-02-19 18:30:00</td>\n",
       "      <td>1950</td>\n",
       "      <td>1951</td>\n",
       "      <td>1951-02-20 00:30:00+00:00</td>\n",
       "      <td>40015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>195102</td>\n",
       "      <td>19</td>\n",
       "      <td>1835</td>\n",
       "      <td>195102</td>\n",
       "      <td>19</td>\n",
       "      <td>1835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10099704</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>35.83</td>\n",
       "      <td>-97.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PUB</td>\n",
       "      <td>1951-02-19 18:35:00</td>\n",
       "      <td>1950</td>\n",
       "      <td>1951</td>\n",
       "      <td>1951-02-20 00:35:00+00:00</td>\n",
       "      <td>40017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>195102</td>\n",
       "      <td>19</td>\n",
       "      <td>2200</td>\n",
       "      <td>195102</td>\n",
       "      <td>19</td>\n",
       "      <td>2200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10099705</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PUB</td>\n",
       "      <td>1951-02-19 22:00:00</td>\n",
       "      <td>1950</td>\n",
       "      <td>1951</td>\n",
       "      <td>1951-02-20 04:00:00+00:00</td>\n",
       "      <td>40027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "65           195103         28         510         195103       28       510   \n",
       "66           195103         30        1500         195103       30      1500   \n",
       "67           195102         19        1830         195102       19      1830   \n",
       "68           195102         19        1835         195102       19      1835   \n",
       "69           195102         19        2200         195102       19      2200   \n",
       "\n",
       "    EPISODE_ID  EVENT_ID         STATE STATE_FIPS  ...  END_LAT END_LON  \\\n",
       "65         NaN  10120421         TEXAS         48  ...      NaN     NaN   \n",
       "66         NaN  10104933  PENNSYLVANIA         42  ...      NaN     NaN   \n",
       "67         NaN  10099493      OKLAHOMA         40  ...    35.37  -98.13   \n",
       "68         NaN  10099704      OKLAHOMA         40  ...    35.83  -97.83   \n",
       "69         NaN  10099705      OKLAHOMA         40  ...      NaN     NaN   \n",
       "\n",
       "   EPISODE_NARRATIVE EVENT_NARRATIVE DATA_SOURCE            BEGIN_DT  \\\n",
       "65               NaN             NaN         PUB 1951-03-28 05:10:00   \n",
       "66               NaN             NaN         PUB 1951-03-30 15:00:00   \n",
       "67               NaN             NaN         PUB 1951-02-19 18:30:00   \n",
       "68               NaN             NaN         PUB 1951-02-19 18:35:00   \n",
       "69               NaN             NaN         PUB 1951-02-19 22:00:00   \n",
       "\n",
       "   WINTER_SEASON SOURCE_FILE_YEAR              BEGIN_DT_UTC COUNTY_FIPS  \n",
       "65          1950             1951 1951-03-28 11:10:00+00:00       48225  \n",
       "66          1950             1951 1951-03-30 21:00:00+00:00       42001  \n",
       "67          1950             1951 1951-02-20 00:30:00+00:00       40015  \n",
       "68          1950             1951 1951-02-20 00:35:00+00:00       40017  \n",
       "69          1950             1951 1951-02-20 04:00:00+00:00       40027  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_winter_scs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d30fec4",
   "metadata": {},
   "source": [
    "### Now to try grouping and binning by County:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cdcf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_counts = (\n",
    "    df_winter_scs.groupby([\"COUNTY_FIPS\",\"EVENT_TYPE\", \"WINTER_SEASON\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)  ##for making wide format\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93f4f4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>WINTER_SEASON</th>\n",
       "      <th>COUNTY_FIPS</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>1950</th>\n",
       "      <th>1951</th>\n",
       "      <th>1952</th>\n",
       "      <th>1953</th>\n",
       "      <th>1954</th>\n",
       "      <th>1955</th>\n",
       "      <th>1956</th>\n",
       "      <th>1957</th>\n",
       "      <th>...</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01000</td>\n",
       "      <td>Hail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01000</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01001</td>\n",
       "      <td>Hail</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01001</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01001</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7274</th>\n",
       "      <td>99127</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>99127</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>99133</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>99139</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>99151</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7279 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "WINTER_SEASON COUNTY_FIPS         EVENT_TYPE  1950  1951  1952  1953  1954  \\\n",
       "0                   01000               Hail     0     0     0     0     0   \n",
       "1                   01000  Thunderstorm Wind     0     0     0     0     0   \n",
       "2                   01001               Hail     0     0     0     0     0   \n",
       "3                   01001  Thunderstorm Wind     0     0     0     0     0   \n",
       "4                   01001            Tornado     0     0     0     0     0   \n",
       "...                   ...                ...   ...   ...   ...   ...   ...   \n",
       "7274                99127  Thunderstorm Wind     0     0     0     0     0   \n",
       "7275                99127            Tornado     0     0     0     0     0   \n",
       "7276                99133  Thunderstorm Wind     0     0     0     0     0   \n",
       "7277                99139            Tornado     0     0     0     0     0   \n",
       "7278                99151  Thunderstorm Wind     0     0     0     0     0   \n",
       "\n",
       "WINTER_SEASON  1955  1956  1957  ...  2015  2016  2017  2018  2019  2020  \\\n",
       "0                 0     0     0  ...     0     0     0     0     0     0   \n",
       "1                 0     0     0  ...     0     0     0     0     0     0   \n",
       "2                 0     0     0  ...     0     0     0     0     0     0   \n",
       "3                 0     0     0  ...     0     1     1     3     2     2   \n",
       "4                 0     0     0  ...     0     0     2     4     0     2   \n",
       "...             ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "7274              0     0     0  ...     0     0     0     0     0     0   \n",
       "7275              0     0     0  ...     0     0     0     0     0     0   \n",
       "7276              0     0     0  ...     0     0     0     0     0     0   \n",
       "7277              0     0     0  ...     0     1     0     0     0     0   \n",
       "7278              0     0     0  ...     0     0     0     0     0     0   \n",
       "\n",
       "WINTER_SEASON  2021  2022  2023  2024  \n",
       "0                 0     0     0     0  \n",
       "1                 0     0     0     0  \n",
       "2                 0     2     0     1  \n",
       "3                 0     1     0     4  \n",
       "4                 0     3     0     1  \n",
       "...             ...   ...   ...   ...  \n",
       "7274              0     0     1     0  \n",
       "7275              0     0     0     0  \n",
       "7276              0     0     1     0  \n",
       "7277              0     0     0     0  \n",
       "7278              0     0     1     0  \n",
       "\n",
       "[7279 rows x 77 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66a1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c40da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
